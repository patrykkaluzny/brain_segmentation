{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_brains",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqbXK-6lYUB4",
        "colab_type": "text"
      },
      "source": [
        "Preaparing virtual enviroment and getting data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA5u4kQDYAye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing virtual enviroment\n",
        "!pip install --upgrade nibabel\n",
        "!rm public.zip\n",
        "!pip install --upgrade nibabel\n",
        "!pip install segmentation-models\n",
        "\n",
        "# get pictures and unpacked them\n",
        "!wget \"https://putpoznanpl-my.sharepoint.com/:u:/g/personal/dominik_pieczynski_put_poznan_pl/EWIZ_xm8wXpMjQDgF2VQ1csB4QuHPKoj5vDpj6CQi9p-AA?e=yQr6fn&download=1\" -O public.zip\n",
        "!unzip -q public.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTxd0SsrYh_M",
        "colab_type": "text"
      },
      "source": [
        "Importing all necessary packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR-Cc1tZYqey",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2d0d467d-ff6b-4aa8-a13f-3f5cebec7aa0"
      },
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from pathlib import Path \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2 as cv\n",
        "from typing import Tuple, List\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from segmentation_models.losses import DiceLoss\n",
        "from tensorflow.keras.models import load_model\n",
        "from segmentation_models.metrics import IOUScore\n",
        "from segmentation_models import Unet\n",
        "import scipy\n",
        "import keras\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Segmentation Models: using `keras` framework.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmHN7x1EZByI",
        "colab_type": "text"
      },
      "source": [
        "All functions used in project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsshnJ3OZG-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions provided by tutor used for loading, saving and visualizing raw data\n",
        "\n",
        "def load_raw_volume(path: Path) -> Tuple[np.ndarray, np.ndarray]:\n",
        "  data: nib.Nifti1Image = nib.load(str(path))\n",
        "  data = nib.as_closest_canonical(data)\n",
        "  raw_data = data.get_fdata(caching='unchanged', dtype=np.float32)\n",
        "  return raw_data, data.affine\n",
        "\n",
        "def load_labels_volume(path: Path) -> np.ndarray:\n",
        "  return load_raw_volume(path)[0].astype(np.uint8)\n",
        "\n",
        "\n",
        "def save_labels(data: np.ndarray, affine: np.ndarray, path: Path):\n",
        "  nib.save(nib.Nifti1Image(data, affine), str(path))\n",
        "\n",
        "\n",
        "def show_slices(slices: List[np.ndarray]):\n",
        "   fig, axes = plt.subplots(1, len(slices))\n",
        "   for i, data_slice in enumerate(slices):\n",
        "       axes[i].imshow(data_slice.T, cmap=\"gray\", origin=\"lower\")\n",
        "\n",
        "# img operations functions\n",
        "\n",
        "def img_normalize(img):\n",
        "  img = img/np.amax(img)*255\n",
        "  return img\n",
        "\n",
        "def get_slice_from_raw_data(raw_data):\n",
        "  return [raw_data[raw_data.shape[0] // 2], raw_data[:, raw_data.shape[1] // 2], raw_data[:, :, raw_data.shape[2] // 2]]\n",
        "\n",
        "def img_padding(img, size):\n",
        "  extended_img = np.zeros(size, dtype=np.uint8)\n",
        "  extended_img[:img.shape[0], :img.shape[1]] = img\n",
        "  return extended_img\n",
        "\n",
        "def img_unpadding(img, size):\n",
        "  return img[:size[0], :size[1]]\n",
        "\n",
        "# drive operations related functions\n",
        "\n",
        "def cerate_storage_folders(paths):\n",
        "  for path in paths:\n",
        "    if not os.path.exists(path):\n",
        "      os.mkdir(path)\n",
        "\n",
        "# saving png to drive functions\n",
        "\n",
        "def save_raw_img_to_drive(slices, saving_path, folders_names, file_name):\n",
        "  \n",
        "  for i in range(3):\n",
        "    current_slice_path = Path(saving_path, folders_names[i], file_name + '.png' )\n",
        "    cv.imwrite(str(current_slice_path), img_normalize(slices[i]), [cv.IMWRITE_PNG_COMPRESSION, 0]) \n",
        "\n",
        "def save_mask_to_drive(slices, saving_path, folders_names, file_name):\n",
        "  \n",
        "  for i in range(3):\n",
        "    current_slice_path = Path(saving_path, folders_names[i], file_name + '.png' )\n",
        "    cv.imwrite(str(current_slice_path), slices[i], [cv.IMWRITE_PNG_COMPRESSION, 0])\n",
        "\n",
        "# dataset operations related functions\n",
        "\n",
        "def get_file_names_from_path(dataset_path):\n",
        "  file_names = []\n",
        "  for scan_path in dataset_path.iterdir():\n",
        "    if not scan_path.name.endswith('mask.nii.gz'):\n",
        "      file_name = os.path.splitext(scan_path.name)\n",
        "      file_name = os.path.splitext(file_name[0])\n",
        "      file_names.append(file_name[0])\n",
        "  return file_names\n",
        "\n",
        "def get_directory_names_from_path(dataset_paths):\n",
        "  directory_names = []\n",
        "  for directory_path in dataset_paths.iterdir():\n",
        "    directory_names.append(directory_path.name)\n",
        "  return directory_names\n",
        "\n",
        "def train_valid_split(file_names):\n",
        "  train_file_names, valid_file_names, temp1, temp2 =  train_test_split(file_names, file_names, test_size = valid_set_percentage, random_state = random_state, shuffle = True)\n",
        "  return train_file_names, valid_file_names\n",
        "\n",
        "def pad_all_data(path, size, datasets_folders_names):\n",
        "  for directory_path in path.iterdir():\n",
        "    if directory_path.name in datasets_folders_names:\n",
        "      for directory_name in directory_path.iterdir():\n",
        "        for img_path in directory_name.iterdir():\n",
        "\n",
        "          try:\n",
        "              img = cv.imread(str(img_path), 0)\n",
        "          except Exception as e:\n",
        "              continue\n",
        "          cv.imwrite(str(img_path), img_padding(img, size))\n",
        "\n",
        "# predictions functions\n",
        "\n",
        "def get_prediction(data_slice, model, padding_shape):\n",
        "  data_slice = img_normalize(data_slice)\n",
        "  size = data_slice.shape[:2]\n",
        "  data_slice = img_padding(data_slice, padding_shape)\n",
        "  predict_result = model.predict(data_slice[None, :])\n",
        "  predict_result = predict_result.squeeze()\n",
        "  predict_result[predict_result > 0.5] = 1\n",
        "  predict_result[predict_result != 1] = 0\n",
        "  return img_unpadding(predict_result, size)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_9mGVCTdEvR",
        "colab_type": "text"
      },
      "source": [
        "Variables used in project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwXGiQdOdINT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths variables\n",
        "\n",
        "main_path = '/content/drive/My Drive/zpo_project'\n",
        "\n",
        "main_folders_names = ['data', 'models', 'results']\n",
        "datasets_folders_names = ['train', 'valid']\n",
        "scans_folders_names = ['imgs_side','imgs_front','imgs_top']\n",
        "masks_folders_names = ['masks_side','masks_front','masks_top']\n",
        "predict_folders_names = ['result_1', 'result_2']\n",
        "\n",
        "first_dataset_path = Path('/content/FirstDataset/')\n",
        "second_dataset_path = Path('/content/SecondDataset/')\n",
        "\n",
        "# used in first dataset\n",
        "mask_ext = '_mask.nii.gz'\n",
        "scan_ext = '.nii.gz'\n",
        "\n",
        "# used in second dataset\n",
        "mask_file_name = 'mask.nii.gz'\n",
        "scan_file_name = 'T1w.nii.gz'\n",
        "\n",
        "valid_set_percentage = 0.15\n",
        "random_state = 42\n",
        "\n",
        "padding_shape = (512, 512)\n",
        "\n",
        "input_shape = (512, 512, 1)\n",
        "\n",
        "loss = DiceLoss()\n",
        "metric = IOUScore()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l9s183YrWws",
        "colab_type": "text"
      },
      "source": [
        "Prepare drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6skjMWnTrWRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "93656ba8-cc8e-4032-89ef-314d329f4f49"
      },
      "source": [
        "# mount drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# create necessary folders\n",
        "paths = ['/content/drive/My Drive/zpo_project',\n",
        "         '/content/drive/My Drive/zpo_project/data',\n",
        "         '/content/drive/My Drive/zpo_project/data/train',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/imgs_side',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/imgs_front',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/imgs_top',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/masks_side',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/masks_front',\n",
        "         '/content/drive/My Drive/zpo_project/data/train/masks_top',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/imgs_side',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/imgs_front',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/imgs_top',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/masks_side',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/masks_front',\n",
        "         '/content/drive/My Drive/zpo_project/data/valid/masks_top',\n",
        "         '/content/drive/My Drive/zpo_project/models',\n",
        "         '/content/drive/My Drive/zpo_project/results',\n",
        "         '/content/drive/My Drive/zpo_project/results/',\n",
        "         '/content/drive/My Drive/zpo_project/results/result_1',\n",
        "         '/content/drive/My Drive/zpo_project/results/result_2'\n",
        "         ]\n",
        "cerate_storage_folders(paths)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TYVj4lXEjRC",
        "colab_type": "text"
      },
      "source": [
        "Preparing raw data for training (padding and saving as png on drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYDbb6skEi56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_save_path = Path('/content/drive/My Drive/zpo_project/data/train')\n",
        "valid_save_path = Path('/content/drive/My Drive/zpo_project/data/valid')\n",
        "\n",
        "# first dataset \n",
        "\n",
        "train_path = os.path.join(first_dataset_path, 'train')\n",
        "file_names = get_file_names_from_path(Path(train_path))\n",
        " \n",
        "train_file_names, valid_file_names = train_valid_split(file_names)\n",
        "\n",
        "for file_name in train_file_names:\n",
        "\n",
        "  raw_volume = None\n",
        "  mask_volume = None\n",
        "  mask_path = os.path.join(train_path, file_name+mask_ext)\n",
        "  scan_path = os.path.join(train_path, file_name+scan_ext)\n",
        " \n",
        "  # load mask and scan file, check for errors\n",
        "  try:\n",
        "    mask_volume = load_labels_volume(mask_path)\n",
        "    raw_volume, affine = load_raw_volume(scan_path)\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "  # if loaded, get images into list\n",
        "  scans = get_slice_from_raw_data(raw_volume)\n",
        "  masks = get_slice_from_raw_data(mask_volume)\n",
        " \n",
        "  save_raw_img_to_drive(scans, train_save_path, scans_folders_names, file_name)\n",
        "  save_mask_to_drive(masks, train_save_path, masks_folders_names, file_name)\n",
        "\n",
        "for file_name in valid_file_names:\n",
        " \n",
        "  raw_volume = None\n",
        "  mask_volume = None\n",
        "  mask_path = os.path.join(train_path, file_name+mask_ext)\n",
        "  scan_path = os.path.join(train_path, file_name+scan_ext)\n",
        " \n",
        "  # load mask and scan file, check for errors\n",
        "  try:\n",
        "    mask_volume = load_labels_volume(mask_path)\n",
        "    raw_volume, affine = load_raw_volume(scan_path)\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "  # if loaded, get images into list\n",
        "  scans = get_slice_from_raw_data(raw_volume)\n",
        "  masks = get_slice_from_raw_data(mask_volume)\n",
        " \n",
        "  save_raw_img_to_drive(scans, valid_save_path, scans_folders_names, file_name)\n",
        "  save_mask_to_drive(masks, valid_save_path, masks_folders_names, file_name)\n",
        "\n",
        "# second dataset \n",
        "train_path = os.path.join(second_dataset_path, 'train')\n",
        "dirs_names = get_directory_names_from_path(Path(train_path))\n",
        "\n",
        "train_dirs_names, valid_dirs_names = train_valid_split(dirs_names)\n",
        "\n",
        "for dir_name in train_dirs_names:\n",
        "\n",
        "  raw_volume = None\n",
        "  mask_volume = None\n",
        "  mask_path = os.path.join(train_path, dir_name, mask_file_name)\n",
        "  scan_path = os.path.join(train_path, dir_name, scan_file_name)\n",
        "\n",
        "   # load mask and scan file\n",
        "  try:\n",
        "    mask_volume = load_labels_volume(mask_path)\n",
        "    raw_volume, affine = load_raw_volume(scan_path)\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "  scans = get_slice_from_raw_data(raw_volume)\n",
        "  masks = get_slice_from_raw_data(mask_volume)\n",
        " \n",
        "  save_raw_img_to_drive(scans, train_save_path, scans_folders_names, dir_name)\n",
        "  save_mask_to_drive(masks, train_save_path, masks_folders_names, dir_name)\n",
        "\n",
        "for dir_name in valid_dirs_names:\n",
        "\n",
        "  raw_volume = None\n",
        "  mask_volume = None\n",
        "  mask_path = os.path.join(train_path, dir_name, mask_file_name)\n",
        "  scan_path = os.path.join(train_path, dir_name, scan_file_name)\n",
        "\n",
        "   # load mask and scan file\n",
        "  try:\n",
        "    mask_volume = load_labels_volume(mask_path)\n",
        "    raw_volume, affine = load_raw_volume(scan_path)\n",
        "  except:\n",
        "    continue\n",
        "  \n",
        "  scans = get_slice_from_raw_data(raw_volume)\n",
        "  masks = get_slice_from_raw_data(mask_volume)\n",
        " \n",
        "  save_raw_img_to_drive(scans, valid_save_path, scans_folders_names, dir_name)\n",
        "  save_mask_to_drive(masks, valid_save_path, masks_folders_names, dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGnRVLsyPYmf",
        "colab_type": "text"
      },
      "source": [
        "Applay padding to make all images same size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne39Uk_uPeTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding\n",
        "data_path = Path('/content/drive/My Drive/zpo_project/data')\n",
        "pad_all_data(data_path, padding_shape, datasets_folders_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObngnLanNuco",
        "colab_type": "text"
      },
      "source": [
        "Deep learning section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvVT_edXN1oH",
        "colab_type": "text"
      },
      "source": [
        "Prepare virtual machine \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur_qgiDQN5Wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "1a5ffaa7-db14-4017-c3a9-9159d97ed766"
      },
      "source": [
        "# loading training data to colab virtual machine\n",
        "\n",
        "# Train dataset\n",
        "!mkdir /content/train\n",
        "!mkdir /content/valid\n",
        "!mkdir /content/test\n",
        "\n",
        "!mkdir /content/train/img_front\n",
        "!mkdir /content/train/mask_front\n",
        "!mkdir /content/train/img_front/all_img\n",
        "!mkdir /content/train/mask_front/all_mask\n",
        "\n",
        "!mkdir /content/train/img_side\n",
        "!mkdir /content/train/mask_side\n",
        "!mkdir /content/train/img_side/all_img\n",
        "!mkdir /content/train/mask_side/all_mask\n",
        "\n",
        "!mkdir /content/train/img_top\n",
        "!mkdir /content/train/mask_top\n",
        "!mkdir /content/train/img_top/all_img\n",
        "!mkdir /content/train/mask_top/all_mask\n",
        "\n",
        "!mkdir /content/valid/img_front\n",
        "!mkdir /content/valid/mask_front\n",
        "!mkdir /content/valid/img_front/all_img\n",
        "!mkdir /content/valid/mask_front/all_mask\n",
        "\n",
        "!mkdir /content/valid/img_side\n",
        "!mkdir /content/valid/mask_side\n",
        "!mkdir /content/valid/img_side/all_img\n",
        "!mkdir /content/valid/mask_side/all_mask\n",
        "\n",
        "!mkdir /content/valid/img_top\n",
        "!mkdir /content/valid/mask_top\n",
        "!mkdir /content/valid/img_top/all_img\n",
        "!mkdir /content/valid/mask_top/all_mask\n",
        "\n",
        "!mkdir /content/test/img_front\n",
        "!mkdir /content/test/img_front/all_img\n",
        "\n",
        "!mkdir /content/test/img_side\n",
        "!mkdir /content/test/img_side/all_img\n",
        "\n",
        "!mkdir /content/test/img_top\n",
        "!mkdir /content/test/img_top/all_img\n",
        "\n",
        "# Copying to virtual machine\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/imgs_front/*.png /content/train/img_front/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/masks_front/*.png /content/train/mask_front/all_mask\n",
        "\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/imgs_front/*.png /content/valid/img_front/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/masks_front/*.png /content/valid/mask_front/all_mask\n",
        "\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/imgs_side/*.png /content/train/img_side/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/masks_side/*.png /content/train/mask_side/all_mask\n",
        "\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/imgs_side/*.png /content/valid/img_side/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/masks_side/*.png /content/valid/mask_side/all_mask\n",
        "\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/imgs_top/*.png /content/train/img_top/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/train/masks_top/*.png /content/train/mask_top/all_mask\n",
        "\n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/imgs_top/*.png /content/valid/img_top/all_img \n",
        "!cp /content/drive/My\\ Drive/zpo_project/data/valid/masks_top/*.png /content/valid/mask_top/all_mask"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/train’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid’: File exists\n",
            "mkdir: cannot create directory ‘/content/test’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_front’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_front’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_front/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_front/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_side’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_side’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_side/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_side/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_top’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_top’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/img_top/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/train/mask_top/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_front’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_front’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_front/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_front/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_side’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_side’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_side/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_side/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_top’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_top’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/img_top/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/valid/mask_top/all_mask’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_front’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_front/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_side’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_side/all_img’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_top’: File exists\n",
            "mkdir: cannot create directory ‘/content/test/img_top/all_img’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlBjdqKrURp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables for traning \n",
        "\n",
        "imgs_train = ['/content/train/img_front', '/content/train/img_top', '/content/train/img_side']\n",
        "masks_train = ['/content/train/mask_front', '/content/train/mask_top', '/content/train/mask_side']\n",
        "\n",
        "imgs_valid = ['/content/valid/img_front', '/content/valid/img_top', '/content/valid/img_side']\n",
        "masks_valid = ['/content/valid/mask_front', '/content/valid/mask_top',  '/content/valid/mask_side']\n",
        "model_paths = ['/content/drive/My Drive/zpo_project/models/side.h5',\n",
        "               '/content/drive/My Drive/zpo_project/models/front.h5', \n",
        "               '/content/drive/My Drive/zpo_project/models/top.h5']\n",
        "\n",
        "target_size = padding_shape\n",
        "train_batch_size = 8\n",
        "valid_batch_size = 16\n",
        "random_seed = 42\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alt4-MXJVsnt",
        "colab_type": "text"
      },
      "source": [
        "Generators and traning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tazq0hpUVxM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "b42a4ddb-70c3-49c2-e997-464c13282457"
      },
      "source": [
        "for i in range(len(model_paths)):\n",
        "\n",
        "  # generate generators\n",
        "  img_gen_train = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "  masks_gen_train = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  img_gen_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  masks_gen_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "  train_images_generator = img_gen_train.flow_from_directory(\n",
        "      imgs_train[i],\n",
        "      target_size=target_size,\n",
        "      batch_size=train_batch_size,\n",
        "      class_mode=None,\n",
        "      seed=random_seed,\n",
        "      color_mode=\"grayscale\"\n",
        "  )\n",
        "\n",
        "  train_masks_generator = masks_gen_train.flow_from_directory(\n",
        "      masks_train[i],\n",
        "      target_size=target_size,\n",
        "      batch_size=train_batch_size,\n",
        "      class_mode=None,\n",
        "      seed=random_seed,\n",
        "      color_mode=\"grayscale\"\n",
        "  )\n",
        "\n",
        "  val_images_generator = img_gen_val.flow_from_directory(\n",
        "      imgs_valid[i],\n",
        "      target_size=target_size,\n",
        "      batch_size=valid_batch_size,\n",
        "      class_mode=None,\n",
        "      seed=random_seed,\n",
        "      color_mode=\"grayscale\"\n",
        "  )\n",
        "\n",
        "  val_masks_generator = masks_gen_val.flow_from_directory(\n",
        "      masks_valid[i],\n",
        "        target_size=target_size,\n",
        "      batch_size=valid_batch_size,\n",
        "      class_mode=None,\n",
        "      seed=random_seed,\n",
        "      color_mode=\"grayscale\"\n",
        "  )\n",
        "\n",
        "  train_combined_generator = zip(train_images_generator, train_masks_generator)\n",
        "  val_combined_generator = zip(val_images_generator, val_masks_generator)\n",
        "\n",
        "  # load model and train it\n",
        "  training_samples = train_images_generator.n\n",
        "  validation_samples = val_images_generator.n\n",
        "  opt = keras.optimizers.Adam(lr=1e-3)\n",
        "  metric = IOUScore()\n",
        "  model = Unet('inceptionv3', input_shape=input_shape, encoder_weights=None)\n",
        "  model.compile(optimizer=opt, loss=loss,\n",
        "                    metrics=[metric])\n",
        "\n",
        "  history = model.fit_generator(\n",
        "    train_combined_generator,\n",
        "    steps_per_epoch=training_samples // 8,\n",
        "    epochs=20,\n",
        "    validation_data=val_combined_generator, \n",
        "    validation_steps=validation_samples // 16\n",
        "  )\n",
        "\n",
        "  model.save(model_paths[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 380 images belonging to 1 classes.\n",
            "Found 380 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Epoch 1/1\n",
            "47/47 [==============================] - 152s 3s/step - loss: 0.5753 - iou_score: 0.2891 - val_loss: 0.9848 - val_iou_score: 0.0044\n",
            "Found 380 images belonging to 1 classes.\n",
            "Found 380 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Epoch 1/1\n",
            "47/47 [==============================] - 131s 3s/step - loss: 0.5572 - iou_score: 0.3043 - val_loss: 0.8271 - val_iou_score: 0.0922\n",
            "Found 380 images belonging to 1 classes.\n",
            "Found 380 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Found 111 images belonging to 1 classes.\n",
            "Epoch 1/1\n",
            "47/47 [==============================] - 131s 3s/step - loss: 0.6932 - iou_score: 0.1871 - val_loss: 0.8315 - val_iou_score: 0.0954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h_2acukdZEk",
        "colab_type": "text"
      },
      "source": [
        "Make predictions and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blT8s5X0f_VI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare variables\n",
        "\n",
        "\n",
        "models_paths = ['/content/drive/My Drive/zpo_project/models/side.h5',\n",
        "               '/content/drive/My Drive/zpo_project/models/front.h5', \n",
        "               '/content/drive/My Drive/zpo_project/models/top.h5']\n",
        "\n",
        "result_1_path = Path('/content/drive/My Drive/zpo_project/results/result_1')\n",
        "result_2_path = Path('/content/drive/My Drive/zpo_project/results/result_2')\n",
        "\n",
        "models = []\n",
        "for model in models_paths:  \n",
        "  models.append(load_model(model, custom_objects={'dice_loss': loss, 'iou_score': metric}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEvULgmEdc2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0c77c89-636d-4196-a15a-8700fce9565f"
      },
      "source": [
        "# first data set prediction\n",
        "\n",
        "test_data_path = Path('/content/FirstDataset/test')\n",
        "\n",
        "for scan_path in test_data_path.iterdir():\n",
        "\n",
        "  data, affine = load_raw_volume(scan_path)\n",
        "  labels = np.zeros(data.shape, dtype=np.float32)\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "    labels[i] += get_prediction(data[i], models[0], padding_shape)\n",
        "  for i in range(data.shape[1]):\n",
        "    labels[:,i] += get_prediction(data[:,i], models[1], padding_shape)\n",
        "  for i in range(data.shape[2]):\n",
        "    labels[:,:,i] += get_prediction(data[:,:,i], models[2], padding_shape)\n",
        "\n",
        "  labels = labels/3\n",
        "  labels[labels > 0.7] = 1\n",
        "  labels[labels != 1] = 0\n",
        "\n",
        "  save_labels(labels, affine, os.path.join(result_1_path, scan_path.name))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMl-To5qgagy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# second data set prediction\n",
        "\n",
        "test_data_path = Path('/content/SecondDataset/test')\n",
        "\n",
        "for scan_path in test_data_path.iterdir():\n",
        "\n",
        "  data, affine = load_raw_volume(os.path.join(scan_path,'T1w.nii.gz'))\n",
        "  labels = np.zeros(data.shape, dtype=np.float32)\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "    labels[i] += get_prediction(data[i], models[0], padding_shape)\n",
        "  for i in range(data.shape[1]):\n",
        "    labels[:,i] += get_prediction(data[:,i], models[1], padding_shape)\n",
        "  for i in range(data.shape[2]):\n",
        "    labels[:,:,i] += get_prediction(data[:,:,i], models[2], padding_shape)\n",
        "\n",
        "\n",
        "  labels = labels/3\n",
        "  labels[labels > 0.7] = 1\n",
        "  labels[labels != 1] = 0\n",
        "\n",
        "  save_labels(labels, affine, os.path.join(result_2_path, scan_path.name+'.nii.gz'))\n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FI5sAOaZf_o",
        "colab_type": "text"
      },
      "source": [
        "Checking results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hRfsZHuZkKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import zlib\n",
        "\n",
        "for dataset_predictions_path in (result_1_path, result_2_path):\n",
        "  for prediction_path in dataset_predictions_path.iterdir():\n",
        "    prediction_name = prediction_path.name[:-7]  # Usuwanie '.nii.gz' z nazwy pliku\n",
        "    prediction = nib.load(str(prediction_path))\n",
        "\n",
        "    response = requests.post(f'http://vision.dpieczynski.pl:8080/{prediction_name}', data=zlib.compress(prediction.to_bytes()))\n",
        "    if response.status_code == 200:\n",
        "        print(dataset_predictions_path.name, prediction_path.name, response.json())\n",
        "    else:\n",
        "        print(f'Error processing prediction {dataset_predictions_path.name}/{prediction_name}: {response.text}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}